In class exercise:

Bagging is a special case of random forests under which case?

When n = p
Suppose we have n observations in our data set, but we want to predict on a new set, we would use bagging. 

This is because collecting a complete new data sets is not practical, and splitting our single data set into n parts would also not 
be a substitute, as we'd only be skyrocketing the variance of each tree before then reducing it in the ensemble.

What are the hyperparameters we can control for random forests?

when making a new stump to grow a tree, we choose one predictor out of the total p predictors. 
The idea behind random forests is to restrict this choice to some random subset of m predictors out of the p. 
A new batch of m predictors is selected each time a stump is to be made.
e.g.
number of branches (depth)
number of trees

Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?

(1,0), (1,2), (1,5) NO, as there is no (1,0) in the original data set

(1,2), (2,0) NO, there are only two in this data set, we need 3 observations

(1,2), (1,2), (1,5) YES, these are valid observations from the orginal data set

For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)?
OOB in (1,2), (1,2), (1,5) is (2,0)

You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. 
What would your prediction be in the following cases?
Regression: your trees make the following four predictions: 1,1,3,3.
2 would be my prediction --> simply use the average

Classification: your trees make the following four predictions: "A", "A", "B", "C".
"A" would be my prediction as this is the one shows up most frequently. 


